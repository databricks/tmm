{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab296dff-1428-489e-869f-73f4bd208278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## AI Governance Lab: Building Secure HR Agents with Databricks\n",
    "\n",
    "In this hands-on lab, you'll learn how to build AI agents that respect data governance policies using Databricks Unity Catalog. We'll create an HR Analyst agent that can answer questions about employees, compensation, and policies while automatically enforcing security controls through view-based access control and data classification.\n",
    "\n",
    "### Why This Matters\n",
    "As organizations deploy AI agents with access to sensitive data, governance becomes critical. Without proper controls, an AI agent could inadvertently expose confidential information like salaries, SSNs, or HR cases. This lab demonstrates how to build agents that are both helpful and secure using a layered security approach with data classifications.\n",
    "\n",
    "### Prerequisites Completed ‚úì\n",
    "- Unity Catalog enabled\n",
    "- HR tables created and classified in `clientcare.hr_data`:\n",
    "  - `employee_records` (Confidential) - Contains PII\n",
    "  - `compensation_data` (Restricted) - Contains salary information\n",
    "  - `performance_reviews` (Confidential) - Contains employee assessments\n",
    "  - `hr_cases` (Restricted) - Contains sensitive HR matters\n",
    "  - `public_policies` (Public) - Available to all\n",
    "  - `internal_procedures` (Internal) - For employees only\n",
    "- Group `hr_data_analysts` was pre-created in the environment\n",
    "\n",
    "### Lab Structure\n",
    "**Lab 1: Building the Governance Foundation**\n",
    "1. Apply Data Classifications to tables\n",
    "2. Create Classification-Aware Views with anonymization and filtering\n",
    "3. Create Security Group for HR analytics access\n",
    "4. Grant Group Permissions on views and functions\n",
    "5. Implement Table-Level Column Masking for SSN\n",
    "6. Build Secure Tools (UC Functions) for the agent\n",
    "7. Test the complete governance stack\n",
    "\n",
    "**Lab 2: Implementing the AI Agent**\n",
    "1. Build the HR Analyst agent using the secure tools from Lab 1\n",
    "2. Register the agent model in MLflow\n",
    "3. Deploy agent as model endpoint\n",
    "4. Verify agent permissions through group membership\n",
    "5. Test agent with governance-aware queries\n",
    "6. Evaluate traces to validate security controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d80afd-082f-4de7-bb7f-874973dcda38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Initial Setup and Data Verification**\n",
    "\n",
    "Let's start by verifying our environment and examining the data classifications we've already applied.\n",
    "\n",
    "_Background for those new to Databricks_\n",
    "\n",
    "- We use `spark.sql()` for all governance OPERATIONS (CREATE VIEW, GRANT, etc.)\n",
    "- We use `pandas.DataFrame()` for showing results in a nice format\n",
    "- We use the Unity Catalog SDK (`WorkspaceClient`) for verification and reading metadata\n",
    "\n",
    "**Implementing Unity Catalog Governance: Available Methods**\n",
    "\n",
    "1. **Pure SQL** - The standard approach for production environments \n",
    "2. **Spark SQL in Python** - SQL commands as strings in Python (what we'll use)\n",
    "3. **PySpark DataFrame API** - The Pythonic approach for queries\n",
    "4. **Databricks SDK** - For reading metadata and automation\n",
    "5. **REST API** - For external integrations and CI/CD pipelines\n",
    "6. **Terraform** - Infrastructure as Code approach for version-controlled, repeatable deployments\n",
    "\n",
    "In this lab, we'll primarily use **Spark SQL in Python** (#2) for governance operations and the **SDK** (#4) for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "785f7ec9-d3eb-4f5b-ba4c-e2facef8962c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# CLEANUP: RESET ALL GOVERNANCE OBJECTS\n",
    "# --------------------------------------------\n",
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "\n",
    "catalog_name = \"clientcare\" \n",
    "schema_name = \"hr_data\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "print(\"üîÑ CLEANING UP ANY EXISTING GOVERNANCE OBJECTS...\")\n",
    "\n",
    "# Remove any direct table masks (if applied from previous runs)\n",
    "try:\n",
    "    spark.sql(\"ALTER TABLE employee_records ALTER COLUMN ssn DROP MASK\")\n",
    "    spark.sql(\"ALTER TABLE employee_records ALTER COLUMN phone DROP MASK\")\n",
    "    spark.sql(\"ALTER TABLE employee_records ALTER COLUMN email DROP MASK\")\n",
    "    spark.sql(\"ALTER TABLE compensation_data ALTER COLUMN base_salary DROP MASK\")\n",
    "    spark.sql(\"ALTER TABLE compensation_data ALTER COLUMN bonus DROP MASK\")\n",
    "    print(\"‚úì Removed any existing column masks\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: No existing masks to remove\")\n",
    "\n",
    "# Drop existing views\n",
    "views_to_drop = [\n",
    "    \"data_analyst_view\",\n",
    "    \"admin_hr_view\",\n",
    "    \"hr_public_view\",\n",
    "    \"hr_internal_view\", \n",
    "    \"hr_confidential_view\",\n",
    "    \"hr_restricted_view\",\n",
    "    \"hr_unified_view\"\n",
    "]\n",
    "\n",
    "for view in views_to_drop:\n",
    "    try:\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view}\")\n",
    "        print(f\"‚úì Dropped view {view}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: View {view} didn't exist\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete! Ready to build governance from scratch.\")\n",
    "\n",
    "# Show current state of raw data that needs protection\n",
    "print(\"\\n‚ö†Ô∏è Current UNPROTECTED data:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.employee_id,\n",
    "        e.first_name,\n",
    "        e.ssn,              -- Needs masking!\n",
    "        e.department,\n",
    "        c.base_salary,      -- Needs masking!\n",
    "        c.bonus            -- Needs masking!\n",
    "    FROM employee_records e\n",
    "    JOIN compensation_data c ON e.employee_id = c.employee_id\n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c2fef3-f2b9-4710-9e8d-63fc343ded8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set our working environment\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "w = WorkspaceClient()# Read-only for most operations\n",
    "# Cannot set masks or row filters via SDK\n",
    "\n",
    "catalog_name = \"clientcare\" \n",
    "schema_name = \"hr_data\"\n",
    "\n",
    "# Verify all tables are loaded\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8676ee8-352a-4084-8635-ab3e7d8c6327",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This demonstrates the current security risk - without proper governance controls,\n",
    "# all sensitive data (SSNs, salaries, bonuses) is fully exposed to any user with table access\n",
    "# In a real production environment, this would be a major compliance violation\n",
    "\n",
    "display(spark.sql(\"\"\"\n",
    "   SELECT \n",
    "       e.employee_id,\n",
    "       e.first_name,\n",
    "       e.last_name,\n",
    "       e.department,\n",
    "       e.ssn,\n",
    "       e.phone,\n",
    "       e.email,\n",
    "       e.hire_date,\n",
    "       c.base_salary,\n",
    "       c.bonus,\n",
    "       c.stock_options\n",
    "   FROM employee_records e\n",
    "   JOIN compensation_data c ON e.employee_id = c.employee_id\n",
    "   ORDER BY e.employee_id\n",
    "   LIMIT 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edfeb1df-a03b-4e24-ae79-a5c464eecc25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Apply Data Classifications**\n",
    "\n",
    "First, we'll tag our tables with sensitivity levels. Table properties are metadata tags that help classify data sensitivity without changing the actual data structure:\n",
    "\n",
    "- Metadata key-value pairs attached to tables\n",
    "- Used by governance tools to understand data sensitivity\n",
    "- Don't affect table structure or data\n",
    "- Can be queried programmatically for compliance\n",
    "\n",
    "Our data classification schema:\n",
    " - Public: Anyone can access (company policies)\n",
    " - Internal: Employees only (procedures)\n",
    " - Confidential: Limited access (employee records, reviews)\n",
    " - Restricted: Highly sensitive (compensation, HR cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83722680-f1de-41ee-9b21-203d4e65613f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define classification scheme\n",
    "tables_config = [\n",
    "    (\"employee_records\", \"Confidential\", \"true\"),     # Names, SSN, contact info\n",
    "    (\"compensation_data\", \"Restricted\", \"true\"),      # Salaries and bonuses\n",
    "    (\"performance_reviews\", \"Confidential\", \"true\"),  # Employee evaluations\n",
    "    (\"hr_cases\", \"Restricted\", \"true\"),               # Sensitive HR investigations\n",
    "    (\"public_policies\", \"Public\", \"false\"),           # Company handbook\n",
    "    (\"internal_procedures\", \"Internal\", \"false\")      # HR processes\n",
    "]\n",
    "\n",
    "# Apply classifications using spark.sql\n",
    "for table_name, classification, has_pii in tables_config:\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {table_name} SET TBLPROPERTIES (\n",
    "            'data_classification' = '{classification}',\n",
    "            'contains_pii' = '{has_pii}',\n",
    "            'governance_enabled' = 'true'\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(f\"‚úì Classified {table_name} as {classification}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f371db7-574f-481f-adba-96c784ef9932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify using SDK to read properties\n",
    "verification_data = []\n",
    "for table_name, expected_class, expected_pii in tables_config:\n",
    "    try:\n",
    "        # Get table info using SDK\n",
    "        table = w.tables.get(full_name=f\"{catalog_name}.{schema_name}.{table_name}\")\n",
    "        \n",
    "        # Properties are read-only via SDK\n",
    "        properties = table.properties if table.properties else {}\n",
    "        \n",
    "        verification_data.append({\n",
    "            \"Table\": table_name,\n",
    "            \"Classification\": properties.get('data_classification', 'Not set'),\n",
    "            \"Contains PII\": properties.get('contains_pii', 'Not set'),\n",
    "            \"Governance Enabled\": properties.get('governance_enabled', 'Not set')\n",
    "        })\n",
    "    except Exception as e:\n",
    "        verification_data.append({\n",
    "            \"Table\": table_name,\n",
    "            \"Classification\": f\"Error: {str(e)}\",\n",
    "            \"Contains PII\": \"-\",\n",
    "            \"Governance Enabled\": \"-\"\n",
    "        })\n",
    "\n",
    "display(pd.DataFrame(verification_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8035047e-c807-4cff-b403-8a19be3fb434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Agent Permissions & Agent Access Requirements**\n",
    "Now that we've classified our data, let's design the right access level for our HR analytics agent through purpose-built views.\n",
    "\n",
    "**Key Decision:** The agent needs a specialized view - \"Data Analyst View\" - that provides analytical capabilities while protecting individual privacy.\n",
    "\n",
    "What the Agent Needs vs. Doesn't Need:\n",
    "| Data Type | HR Admin Sees | Manager Sees | Data Analyst Agent | Why? |\n",
    "|-----------|---------------|--------------|------------------|------|\n",
    "| **Names** | ‚úÖ John Smith | ‚úÖ John Smith | ‚ùå Anonymous IDs | Prevents bias, protects privacy |\n",
    "| **SSN** | ‚úÖ 123-45-6789 | ‚ùå Hidden | ‚ùå Hidden | No analytical value |\n",
    "| **Salary** | ‚úÖ $120,000 | ‚ùå Hidden | ‚úÖ $120,000 | Needed for accurate statistics |\n",
    "| **Department** | ‚úÖ Engineering | ‚úÖ Engineering | ‚úÖ Engineering | Needed for grouping |\n",
    "| **Performance** | ‚úÖ 4.5 | ‚úÖ 4.5 | ‚úÖ 4.5 | Needed for correlation analysis |\n",
    "| **Email** | ‚úÖ john@company.com | ‚úÖ john@company.com | ‚ùå Hidden | No analytical value |\n",
    "| **Phone** | ‚úÖ 555-1234 | ‚úÖ 555-1234 | ‚ùå Hidden | No analytical value |\n",
    "\n",
    "The Agent's Mission:\n",
    "Answer questions like:\n",
    "- \"What's the salary distribution in Engineering?\"\n",
    "- \"Is there pay equity across departments?\"\n",
    "- \"What's the correlation between performance and compensation?\"\n",
    "\n",
    "WITHOUT being able to answer:\n",
    "- \"What's John Smith's salary?\"\n",
    "- \"Who are the top 5 highest paid employees?\"\n",
    "- \"Show me SSNs for employees making over $100k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda895f3-27d3-4598-a31e-a0e9bf9c5025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Create Classification-Aware Views**\n",
    "\n",
    "Now we'll create views that automatically provide appropriate data access based on our classifications. These views serve as the foundation for access control in Databricks.\n",
    "\n",
    "**Why Views for Access Control:**\n",
    "- **Simplicity**: Instead of complex masking rules for every possible user type, create purpose-built views\n",
    "- **Clarity**: Each view has a clear business purpose and user type\n",
    "- **Maintainability**: Easier to understand and modify than intricate permission matrices\n",
    "- **Performance**: Views are optimized SQL - no runtime masking overhead\n",
    "- **Security Layer**: In Databricks, views act as your security layer since we grant permissions directly to principals\n",
    "\n",
    "**Our View Strategy:**\n",
    "We're building two focused views that align with our actual use cases:\n",
    "1. **Data Analyst View**: Anonymous IDs + full compensation data for statistical analysis (designed for AI agents and analysts)\n",
    "2. **Admin View**: Complete unmasked access for HR administrators\n",
    "\n",
    "Rather than trying to anticipate every possible access pattern, we're creating views that solve our specific governance requirements efficiently.\n",
    "\n",
    "**üìù Note: Why No Roles?**\n",
    "If you're coming from traditional databases, you might expect to use `CREATE ROLE`. Databricks Unity Catalog works differently:\n",
    "- **No CREATE ROLE**: This SQL statement doesn't exist in Databricks\n",
    "- **Groups**: Managed at the account/workspace level (we'll use the pre-created `hr_data_analysts` group)\n",
    "- **Direct Grants**: Permissions are granted to users, groups, or service principals\n",
    "- **Why it works**: The views themselves act as your \"roles\" by defining what each access level sees\n",
    "\n",
    "Next, we'll configure the `hr_data_analysts` group with appropriate permissions on these views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f548f991-e750-45eb-80b8-1c8551dcfe9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DATA ANALYST VIEW - For statistical analysis (will be assigned to agent service principal in Lab 2)\n",
    "# Key features: Anonymous employee IDs, full salary access, excludes Legal dept\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW data_analyst_view AS\n",
    "SELECT \n",
    "    CONCAT('EMP_', LPAD(e.employee_id, 6, '0')) as anonymous_id,  -- EMP_000001 format\n",
    "    e.department,\n",
    "    YEAR(e.hire_date) as hire_year,                            -- Year only for better anonymization\n",
    "    c.base_salary,                                              -- Full salary for analytics\n",
    "    c.bonus,\n",
    "    c.stock_options,\n",
    "    YEAR(c.effective_date) as comp_year,\n",
    "    pr.rating,\n",
    "    QUARTER(pr.review_date) as review_quarter,\n",
    "    YEAR(pr.review_date) as review_year\n",
    "    -- Removed pr.comments to prevent identifying information\n",
    "FROM employee_records e\n",
    "LEFT JOIN compensation_data c ON e.employee_id = c.employee_id\n",
    "LEFT JOIN performance_reviews pr ON e.employee_id = pr.employee_id\n",
    "WHERE e.department != 'Legal'  -- Exclude Legal for compliance reasons\n",
    "\"\"\")\n",
    "print(\"‚úì Created data_analyst_view: Anonymous IDs + full compensation data (enhanced anonymization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e7ba88-abd8-462a-97aa-44ea2c001f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ADMIN VIEW - Complete access for HR administrators\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW admin_hr_view AS\n",
    "SELECT \n",
    "    e.employee_id,\n",
    "    e.first_name,\n",
    "    e.last_name,\n",
    "    e.department,\n",
    "    e.email,\n",
    "    e.ssn,\n",
    "    e.phone,\n",
    "    e.hire_date,\n",
    "    e.manager_id,\n",
    "    c.base_salary,\n",
    "    c.bonus,\n",
    "    c.stock_options,\n",
    "    c.effective_date,\n",
    "    pr.rating,\n",
    "    pr.review_date,\n",
    "    pr.comments,\n",
    "    hr.case_id,\n",
    "    hr.case_type,\n",
    "    hr.status,\n",
    "    hr.opened_date,\n",
    "    hr.closed_date,\n",
    "    hr.severity,\n",
    "    hr.description\n",
    "FROM employee_records e\n",
    "LEFT JOIN compensation_data c ON e.employee_id = c.employee_id\n",
    "LEFT JOIN performance_reviews pr ON e.employee_id = pr.employee_id\n",
    "LEFT JOIN hr_cases hr ON e.employee_id = hr.employee_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Created admin_hr_view: Complete access to all HR data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbba1b3b-ffa8-4ce0-8884-60553e8e25b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Configure Group Permissions**\n",
    "\n",
    "Now we'll grant permissions to the pre-created `hr_data_analysts` group. This group will be used by:\n",
    "- Your user account (for testing in this lab)\n",
    "- The AI agent's service principal (in production deployments)\n",
    "- Any data analysts who need access to anonymized HR data\n",
    "\n",
    "This implements the principle of least privilege - the group only gets access to anonymized, aggregated data, never raw employee records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eef6201-6c46-4283-8578-0b7845daf1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify the hr_data_analysts group exists\n",
    "groups_df = spark.sql(\"SHOW GROUPS\")\n",
    "display(groups_df)\n",
    "print(\"‚úÖ Confirmed: hr_data_analysts group is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664284ee-2972-4f17-b6bf-a999bf192249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confirm views in the schema\n",
    "print(\"üìã Verifying our governance views were created successfully...\")\n",
    "print(\"\\nViews in clientcare.hr_data:\")\n",
    "views = spark.sql(\"SHOW VIEWS IN clientcare.hr_data\")\n",
    "display(views)\n",
    "\n",
    "print(\"\\n‚úÖ You should see:\")\n",
    "print(\"   - data_analyst_view (for agents and analysts)\")\n",
    "print(\"   - admin_hr_view (for HR administrators)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a66b5c9-fe0e-466c-a5fb-92dfb69f59b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grant permissions to the hr_data_analysts group using fully qualified names\n",
    "catalog_name = \"clientcare\"\n",
    "schema_name = \"hr_data\"\n",
    "group_name = \"hr_data_analysts\"\n",
    "\n",
    "# Catalog and schema access\n",
    "spark.sql(f\"GRANT USAGE ON CATALOG {catalog_name} TO `{group_name}`\")\n",
    "spark.sql(f\"GRANT USAGE ON SCHEMA {catalog_name}.{schema_name} TO `{group_name}`\")\n",
    "\n",
    "# View access with fully qualified name\n",
    "spark.sql(f\"GRANT SELECT ON VIEW {catalog_name}.{schema_name}.data_analyst_view TO `{group_name}`\")\n",
    "\n",
    "print(f\"‚úÖ Granted permissions to {group_name} group:\")\n",
    "print(f\"   - USAGE on catalog: {catalog_name}\")\n",
    "print(f\"   - USAGE on schema: {catalog_name}.{schema_name}\")\n",
    "print(f\"   - SELECT on view: {catalog_name}.{schema_name}.data_analyst_view\")\n",
    "print(\"\\nüìù Note: We'll grant EXECUTE on UC functions after creating them later in this lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7fd182-6942-4a45-be99-3b360fb62071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Column Masking**\n",
    "**Why implement this when we already have views?**\n",
    "\n",
    "Views can be bypassed if users gain direct table access. Column masking at the table level cannot be bypassed - it's enforced by Unity Catalog on every query, regardless of how the data is accessed.\n",
    "\n",
    "**How Table-Level Security Works:**\n",
    "- **Column Masking**: Functions run automatically transforming sensitive data based on who's viewing it\n",
    "- **Row-Level Security**: Functions can also control which rows users see (we already handled Legal department filtering in our views, but this could be done at the table level too)\n",
    "- Unity Catalog enforces these at the storage layer - users cannot bypass them\n",
    "- Different users see different versions of the same data\n",
    "\n",
    "**Implementation:**\n",
    "We'll mask the SSN field to show data transformation in action. We could also add row-level filtering (like excluding Legal department records), but since we already handled that requirement in our views, we'll focus on column masking.\n",
    "\n",
    "**Production Value:**\n",
    "If someone accidentally grants direct table access or bypasses your views, table-level controls still protect sensitive data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48100f86-f345-482a-90c8-651d9b1d2da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Implement column masking for SSN field - Defense in depth security\n",
    "print(\"üîê Implementing column masking for SSN field...\")\n",
    "\n",
    "# Create column masking function for SSN\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION {catalog_name}.{schema_name}.mask_ssn(ssn_value STRING)\n",
    "RETURNS STRING\n",
    "RETURN CASE \n",
    "    WHEN is_account_group_member('admins') THEN ssn_value\n",
    "    WHEN is_account_group_member('hr_data_analysts') THEN 'ANALYTICS_MASKED'\n",
    "    ELSE CONCAT('***-**-', RIGHT(ssn_value, 4))\n",
    "END\n",
    "\"\"\")\n",
    "\n",
    "# Apply the masking function to the SSN column\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {catalog_name}.{schema_name}.employee_records \n",
    "ALTER COLUMN ssn \n",
    "SET MASK {catalog_name}.{schema_name}.mask_ssn\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Column masking applied to SSN field\")\n",
    "print(\"üîê Defense in Depth: SSN now masked at table level\")\n",
    "print(\"   - Admins see full SSN\")\n",
    "print(\"   - hr_data_analysts group sees 'ANALYTICS_MASKED'\") \n",
    "print(\"   - Other users see '***-**-1234' format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bebf810-7aa3-47d9-b082-e670208fcc5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate column masking implementation\n",
    "print(\"üß™ Testing SSN column masking...\")\n",
    "\n",
    "print(\"\\nüë§ Current user view:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "SELECT employee_id, first_name, last_name, ssn, department \n",
    "FROM {catalog_name}.{schema_name}.employee_records \n",
    "LIMIT 5\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "216543a0-6b5c-4340-a4a6-1e3320df30f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Build Secure Tools for the Agent**\n",
    "Now we'll create Unity Catalog functions that our AI agent will use to query HR data. These functions act as the interface between the agent and our governed data.\n",
    "\n",
    "**Why UC Functions for Agent Tools?**\n",
    "- **Governed Access**: Functions respect the same permissions as the calling principal\n",
    "- **Input Validation**: Can validate and sanitize agent inputs before querying\n",
    "- **Query Templates**: Provide pre-defined, safe query patterns the agent can use\n",
    "- **Audit Trail**: All function calls are logged for compliance\n",
    "- **Performance**: Functions can be optimized with proper indexing and caching\n",
    "\n",
    "**Our Agent Tool Strategy:**\n",
    "We'll create two general-purpose functions that:\n",
    "1. Work exclusively with the anonymized `data_analyst_view`\n",
    "2. Return aggregated results that can answer various HR questions\n",
    "3. Prevent the agent from writing arbitrary SQL\n",
    "4. Maintain employee privacy through anonymous IDs\n",
    "\n",
    "These functions will be the ONLY way our agent interacts with the data, ensuring consistent governance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06c37de7-dcd2-4c98-8c4b-4f696238e661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TOOL 1: Performance & Retention Analytics\n",
    "print(\"üîß Creating performance analytics function...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION\n",
    "    {catalog_name}.{schema_name}.analyze_performance()\n",
    "    RETURNS TABLE (\n",
    "        department STRING,\n",
    "        avg_rating DOUBLE,\n",
    "        min_rating DOUBLE,\n",
    "        max_rating DOUBLE,\n",
    "        employee_count INT,\n",
    "        avg_tenure_years DOUBLE\n",
    "    )\n",
    "    COMMENT 'HR Analytics: Basic performance metrics by department'\n",
    "    RETURN (\n",
    "        SELECT \n",
    "            department,\n",
    "            AVG(rating) as avg_rating,\n",
    "            MIN(rating) as min_rating,\n",
    "            MAX(rating) as max_rating,\n",
    "            COUNT(DISTINCT anonymous_id) as employee_count,\n",
    "            AVG(YEAR(CURRENT_DATE()) - hire_year) as avg_tenure_years\n",
    "        FROM {catalog_name}.{schema_name}.data_analyst_view\n",
    "        WHERE rating IS NOT NULL\n",
    "        GROUP BY department\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Created analyze_performance function\")\n",
    "print(\"  - Returns: avg/min/max ratings, employee count, avg tenure by department\")\n",
    "print(\"  - Works with: anonymized data only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1845ea3f-4a47-42b4-957d-01b411a7c711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TOOL 2: Department & Compensation Analytics\n",
    "print(\"üîß Creating operations analytics function...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION\n",
    "    {catalog_name}.{schema_name}.analyze_operations()\n",
    "    RETURNS TABLE (\n",
    "        department STRING,\n",
    "        employee_count INT,\n",
    "        avg_salary DOUBLE,\n",
    "        avg_bonus DOUBLE,\n",
    "        avg_total_comp DOUBLE,\n",
    "        avg_stock_options INT\n",
    "    )\n",
    "    COMMENT 'HR Analytics: Department compensation and operational metrics'\n",
    "    RETURN (\n",
    "        SELECT \n",
    "            department,\n",
    "            COUNT(DISTINCT anonymous_id) as employee_count,\n",
    "            AVG(base_salary) as avg_salary,\n",
    "            AVG(bonus) as avg_bonus,\n",
    "            AVG(base_salary + bonus) as avg_total_comp,\n",
    "            AVG(stock_options) as avg_stock_options\n",
    "        FROM {catalog_name}.{schema_name}.data_analyst_view\n",
    "        WHERE base_salary IS NOT NULL\n",
    "        GROUP BY department\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Created analyze_operations function\")\n",
    "print(\"  - Returns: compensation metrics and headcount by department\")\n",
    "print(\"  - Maintains privacy: no individual employee data exposed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba620a8-94ad-4b44-a583-01b23de5f399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test 1: Performance Analytics\n",
    "print(\"\\nüìä Test 1: Performance Analytics by Department\")\n",
    "display(spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.analyze_performance()\"))\n",
    "\n",
    "# Test 2: Operations/Compensation Analytics\n",
    "print(\"\\nüìä Test 2: Compensation Analytics by Department\")\n",
    "display(spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.analyze_operations()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd16957-50bd-452d-9eb0-9ede0bbd68d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Governance Foundation Complete\n",
    "\n",
    "### What We've Accomplished:\n",
    "1. **Data Classification**: Applied sensitivity labels (Public, Internal, Confidential, Restricted) to all HR tables\n",
    "\n",
    "2. **Built Classification-Aware Views**: \n",
    "   - `data_analyst_view` - Anonymous IDs, year-only dates, full compensation (designed for AI agents)\n",
    "   - `admin_hr_view` - Complete unmasked access for HR administrators\n",
    "\n",
    "3. **Configured Group-Based Access**:\n",
    "   - Set up `hr_data_analysts` group with permissions on views and functions\n",
    "   - Implemented enterprise-standard permission management\n",
    "   - Ready for both users and service principals\n",
    "\n",
    "4. **Implemented Multi-Layer Security**: \n",
    "   - Table-level SSN masking (cannot be bypassed)\n",
    "   - Row-level filtering (Legal department excluded in views)\n",
    "   - Column-level anonymization (employee IDs ‚Üí EMP_000001 format)\n",
    "\n",
    "5. **Built Secure Agent Tools**:\n",
    "   - `analyze_performance()` - Returns performance ratings and tenure by department\n",
    "   - `analyze_operations()` - Returns compensation metrics and headcount by department\n",
    "   - Both functions granted to `hr_data_analysts` group\n",
    "\n",
    "### Security Architecture Summary:\n",
    "**Raw Tables** ‚Üí **Column Masking** ‚Üí **Views** ‚Üí **UC Functions** ‚Üí **AI Agent**\n",
    "- Column Masking: SSN Protection at table level\n",
    "- Views: Anonymization + Department Filtering  \n",
    "- Groups: Permission management (hr_data_analysts)\n",
    "- UC Functions: Governed access point for agents\n",
    "- AI Agent: Only sees aggregated, anonymous data\n",
    "\n",
    "### Next Steps - Lab 2:\n",
    "1. Build the HR Analyst agent using these secure tools\n",
    "2. Register the agent model in MLflow\n",
    "3. Deploy agent as model endpoint\n",
    "4. Verify agent inherits permissions through group access\n",
    "5. Test agent with governance-aware queries\n",
    "6. Evaluate traces to validate security controls\n",
    "\n",
    "The governance foundation is ready. Let's build the AI agent!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4982940069223228,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "AI_Governance_Lab1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
