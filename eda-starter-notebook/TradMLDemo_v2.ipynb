{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dce7f30-fdae-42ae-8d5b-0e5eb6d59814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bakehouse Sales Forecasting\n",
    "Time series forecasting model using XGBoost to predict next-day product sales by store location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddf67ee-49f0-41f7-8ba9-67d891ad0ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xgboost databricks-feature-engineering --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fc85c39-d41d-4c77-88d6-e84f05e71ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12dc0cc6-e0d2-4815-a91a-036e98750d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transactions = spark.read.table(\"samples.bakehouse.sales_transactions\")\n",
    "franchises = spark.read.table(\"samples.bakehouse.sales_franchises\")\n",
    "display(transactions)\n",
    "display(franchises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "040a72bd-351f-4754-a171-44cead51bb7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2447ebc1-e1b6-4e39-918b-08ad4ac5cf5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Compute daily sales per product per store\n",
    "product_daily_sales = transactions.withColumn(\n",
    "    \"transaction_date\", F.to_date(\"dateTime\")\n",
    ").groupBy(\n",
    "    \"franchiseID\", \"product\", \"transaction_date\"\n",
    ").agg(\n",
    "    F.sum(\"quantity\").alias(\"units_sold\")\n",
    ")\n",
    "\n",
    "# 2. Compute lag features (sales 1 day, 2 days ago, etc.)\n",
    "window_spec = Window.partitionBy(\"franchiseID\", \"product\").orderBy(\"transaction_date\")\n",
    "\n",
    "for lag_day in [1, 2, 3, 7]:\n",
    "    product_daily_sales = product_daily_sales.withColumn(\n",
    "        f\"units_sold_d{lag_day}\",\n",
    "        F.lag(\"units_sold\", lag_day).over(window_spec)\n",
    "    )\n",
    "\n",
    "# 3. Add rolling average of recent sales (last 7 days)\n",
    "product_daily_sales = product_daily_sales.withColumn(\n",
    "    \"mean_7d_units_sold\",\n",
    "    F.avg(\"units_sold\").over(window_spec.rowsBetween(-6, 0))\n",
    ")\n",
    "\n",
    "# 4. Add contextual features: day of week (1=Sunday, ..., 7=Saturday)\n",
    "product_daily_sales = product_daily_sales.withColumn(\n",
    "    \"dow\", F.dayofweek(\"transaction_date\")\n",
    ")\n",
    "display(product_daily_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b2e6824-291c-48e2-8d62-70b7bfc7e951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Window for product/store grouping\n",
    "group_window = Window.partitionBy(\"franchiseID\", \"product\")\n",
    "\n",
    "# List of lag feature columns\n",
    "lag_cols = [\"units_sold_d1\", \"units_sold_d2\", \"units_sold_d3\", \"units_sold_d7\", \"mean_7d_units_sold\"]\n",
    "\n",
    "# For each lag column, create a filled column (fillna with group mean)\n",
    "for col in lag_cols:\n",
    "    mean_col = f\"{col}_mean\"\n",
    "    # Compute the mean for this group\n",
    "    product_daily_sales = product_daily_sales.withColumn(mean_col, F.avg(col).over(group_window))\n",
    "    # Fill NA in lag column with group mean\n",
    "    product_daily_sales = product_daily_sales.withColumn(\n",
    "        col,\n",
    "        F.when(F.col(col).isNull(), F.col(mean_col)).otherwise(F.col(col))\n",
    "    )\n",
    "    # Optionally drop the mean column afterwards to keep dataframe tidy\n",
    "    product_daily_sales = product_daily_sales.drop(mean_col)\n",
    "\n",
    "\n",
    "display(product_daily_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d164503-dccb-488f-81ed-3910a55a40b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In order to predict NEXT DAY sales, we shift \"units_sold\" up by 1 day per group.\n",
    "product_daily_sales = product_daily_sales.withColumn(\n",
    "    \"target_units_sold\", F.lead(\"units_sold\", 1).over(window_spec)\n",
    ")\n",
    "feature_table_df = product_daily_sales.dropna(subset=[\n",
    "    \"units_sold_d1\", \"units_sold_d2\", \"units_sold_d3\",\n",
    "    \"units_sold_d7\", \"mean_7d_units_sold\", \"dow\", \"target_units_sold\"\n",
    "])\n",
    "display(feature_table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27554b4-b0a2-4228-8941-1a3591eab7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the feature table\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "spark.sql(\"DROP DATABASE IF EXISTS workspace.bakehouse_features CASCADE;\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS workspace.bakehouse_features;\")\n",
    "\n",
    "\n",
    "# Create a FeatureEngineeringClient with the desired catalog\n",
    "fe_client = FeatureEngineeringClient()\n",
    "\n",
    "# Now you can use fe_client to work with feature tables in the 'bakehouse' catalog\n",
    "#fe_client = FeatureEngineeringClient(model_registry_uri=\"databricks-uc\")\n",
    "feature_table_name = \"workspace.bakehouse_features.store_product_daily_features\"\n",
    "fe_client.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[\"franchiseID\", \"product\", \"transaction_date\"],\n",
    "    schema=feature_table_df.schema,\n",
    "    description=\"Store-product daily sales with lags and rolling features for time series forecasting\"\n",
    ")\n",
    "fe_client.write_table(name=feature_table_name, df=feature_table_df, mode=\"merge\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71c64f4e-6a1f-48a3-9042-bac9d2d9f01a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1d34e0-ec00-448c-a7da-bb338449262b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "# Convert to Pandas for model training\n",
    "pandas_df = feature_table_df.toPandas()\n",
    "# Define feature columns and target\n",
    "feature_cols = [\"units_sold_d1\", \"units_sold_d2\", \"units_sold_d3\",\n",
    "                \"units_sold_d7\", \"mean_7d_units_sold\", \"dow\"]\n",
    "X = pandas_df[feature_cols]\n",
    "y = pandas_df[\"target_units_sold\"]\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Initialize and train the XGBoost regressor\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "# Make predictions and evaluate\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on test set: {mse}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399219ef-0ab2-4392-83fb-c402fec4b3fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log the model with MLflow\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "signature = infer_signature(X_train, y_train)\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.xgboost.log_model(xgb_model, artifact_path=\"xgboost_model\", signature=signature, input_example=X_test.head())\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    run_id = run.info.run_id\n",
    "print(f\"Model logged in MLflow. Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16bddbaa-5c60-4560-af97-6e74de1e7bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c12e8e-ba70-475a-9ded-882bfe046f3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.bakehouse_ai;\")\n",
    "\n",
    "# Register the model in the Model Registry\n",
    "model_uri = f\"runs:/{run_id}/xgboost_model\"\n",
    "result = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=\"workspace.bakehouse_ai.xgboost_model\"  \n",
    ")\n",
    "version = result.version\n",
    "print(f\"Model {result.name} registered. Model version: {version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e83dde-e406-4bb3-848c-30f02e5604d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a serving endpoint for the registered model\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "endpoint = client.create_endpoint(\n",
    "    name=\"xgboost-model-01\",\n",
    "    config={\n",
    "        \"served_entities\": [{\n",
    "            \"entity_name\": \"workspace.bakehouse_ai.xgboost_model\",\n",
    "            \"entity_version\": version,\n",
    "            \"workload_type\": \"CPU\",\n",
    "            \"workload_size\": \"Small\",\n",
    "            \"scale_to_zero_enabled\": True\n",
    "        }]\n",
    "    }\n",
    ")\n",
    "print(f\"Serving endpoint created: {endpoint.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0019bc-6ecd-4aa2-b62e-cdf8d37f20d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query the endpoint\n",
    "# Format your inputs: dataframe_split is the preferred input style\n",
    "inputs = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": X_test.columns.tolist(),\n",
    "        \"data\": X_test.values.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.predict(endpoint=\"xgboost_model\", inputs=inputs)\n",
    "\n",
    "# The prediction results will be in response['predictions'] or response['outputs']\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d92a27-9530-4b75-a4b3-a90d16a8dced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Cleanup: Remove Feature Table from Feature Store ---\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "fe_client = FeatureEngineeringClient()\n",
    "feature_table_name = \"workspace.bakehouse_features.store_product_daily_features\"\n",
    "try:\n",
    "    fe_client.drop_table(name=feature_table_name)\n",
    "    print(f\"Feature table '{feature_table_name}' dropped.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not drop feature table '{feature_table_name}': {e}\")\n",
    "\n",
    "# --- Cleanup: Drop Feature Database ---\n",
    "try:\n",
    "    spark.sql(\"DROP DATABASE IF EXISTS workspace.bakehouse_features CASCADE;\")\n",
    "    print(\"Dropped database workspace.bakehouse_features.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not drop database: {e}\")\n",
    "\n",
    "# --- Cleanup: Delete Registered Model (from Model Registry) ---\n",
    "import mlflow\n",
    "try:\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    # Find all versions, delete each version first\n",
    "    model_name = \"workspace.bakehouse_ai.xgboost_model\"\n",
    "    versions = [v.version for v in client.get_latest_versions(model_name, stages=[\"None\", \"Staging\", \"Production\"])]\n",
    "    for v in versions:\n",
    "        client.delete_model_version(name=model_name, version=v)\n",
    "        print(f\"Deleted model version {v} for '{model_name}'\")\n",
    "    # Then delete the registered model itself\n",
    "    client.delete_registered_model(model_name)\n",
    "    print(f\"Deleted registered model: {model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not delete registered model: {e}\")\n",
    "\n",
    "# --- Cleanup: Delete Model Serving Endpoint ---\n",
    "from mlflow.deployments import get_deploy_client\n",
    "try:\n",
    "    deploy_client = get_deploy_client(\"databricks\")\n",
    "    deploy_client.delete_endpoint(endpoint=\"xgboost_model\")\n",
    "    print(\"Deleted serving endpoint 'xgboost_model'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not delete serving endpoint: {e}\")\n",
    "\n",
    "# --- Cleanup: Remove Model File from DBFS ---\n",
    "try:\n",
    "    dbutils.fs.rm(\"/dbfs/models/bakery_xgboost_model.joblib\", True)\n",
    "    print(\"Deleted model file /dbfs/models/bakery_xgboost_model.joblib from DBFS.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not delete model file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TradMLDemo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}