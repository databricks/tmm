{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c5bbd3-59fc-4817-a30a-66a96eb2c00e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.streaming.StreamingQuery\n",
    "import play.api.libs.json._\n",
    "import play.api.libs.functional.syntax._\n",
    "import com.databricks.dais2025.SensorDataGenerator\n",
    "import org.apache.spark.sql.execution.streaming.functions.current_batch_id // EDGE\n",
    "\n",
    "object HelperFunctions {\n",
    "\n",
    "  // ---------------------------\n",
    "  // Latency parsing\n",
    "  // ---------------------------\n",
    " case class LatencyMetrics(P0: Long, P50: Long, P90: Long, P95: Long, P99: Long)\n",
    "  case class Latencies(\n",
    "    processingLatencyMs: Option[LatencyMetrics],\n",
    "    sourceQueuingLatencyMs: Option[LatencyMetrics],\n",
    "    e2eLatencyMs: Option[LatencyMetrics]\n",
    "  )\n",
    "  case class StateOperatorMetrics(\n",
    "    operatorName: String,\n",
    "    numRowsTotal: Long,\n",
    "    numRowsUpdated: Long,\n",
    "    numRowsRemoved: Long,\n",
    "    commitTimeMs: Long,\n",
    "    memoryUsedBytes: Long,\n",
    "    customMetrics: Map[String, Long]\n",
    "  )\n",
    "\n",
    "  // JSON readers\n",
    "  implicit val latencyMetricsReads: Reads[LatencyMetrics] = Json.reads[LatencyMetrics]\n",
    "  implicit val latenciesReads: Reads[Latencies] = Json.reads[Latencies]\n",
    "  implicit val stateOperatorMetricsReads: Reads[StateOperatorMetrics] = (\n",
    "    (JsPath \\ \"operatorName\").read[String] and\n",
    "    (JsPath \\ \"numRowsTotal\").readWithDefault[Long](0L) and\n",
    "    (JsPath \\ \"numRowsUpdated\").readWithDefault[Long](0L) and\n",
    "    (JsPath \\ \"numRowsRemoved\").readWithDefault[Long](0L) and\n",
    "    (JsPath \\ \"commitTimeMs\").readWithDefault[Long](0L) and\n",
    "    (JsPath \\ \"memoryUsedBytes\").readWithDefault[Long](0L) and\n",
    "    (JsPath \\ \"customMetrics\").readWithDefault[Map[String, Long]](Map.empty)\n",
    "  )(StateOperatorMetrics.apply _)\n",
    "\n",
    "\n",
    "  // ---------------------------\n",
    "  // Core Functions\n",
    "  // ---------------------------\n",
    "\n",
    "  /** Parses the JSON field from StreamingQueryProgress into Latencies + State Operators */\n",
    "  def parseProgressMetrics(progress: org.apache.spark.sql.streaming.StreamingQueryProgress): (Option[Latencies], Seq[StateOperatorMetrics]) = {\n",
    "    val jsValue = Json.parse(progress.json)\n",
    "\n",
    "    val latenciesOpt = (jsValue \\ \"latencies\").asOpt[Latencies]\n",
    "    val stateOps = (jsValue \\ \"stateOperators\").asOpt[Seq[StateOperatorMetrics]].getOrElse(Seq.empty)\n",
    "\n",
    "    (latenciesOpt, stateOps)\n",
    "  }\n",
    "\n",
    "\n",
    "  /** Pretty prints latency metrics if present */\n",
    "  def printLatencies(latenciesOpt: Option[Latencies]): Unit = {\n",
    "    def fmt(label: String, opt: Option[LatencyMetrics]): String = opt match {\n",
    "      case Some(m) => f\"$label%-28s P50=${m.P50}%-8d P90=${m.P90}%-8d P95=${m.P95}%-8d P99=${m.P99}%-8d\"\n",
    "      case None    => f\"$label%-28s N/A\"\n",
    "    }\n",
    "\n",
    "    latenciesOpt match {\n",
    "      case Some(lat) =>\n",
    "        println(\"Latencies:\")\n",
    "        println(\"  \" + fmt(\"E2E Latency (ms):\", lat.e2eLatencyMs))\n",
    "        println(\"  \" + fmt(\"Processing Latency (ms):\", lat.processingLatencyMs))\n",
    "        println(\"  \" + fmt(\"Source Queuing Latency (ms):\", lat.sourceQueuingLatencyMs))\n",
    "      case None =>\n",
    "        println(\"Latencies: N/A\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /** Main entry point â€” prints everything */\n",
    "  def printRTMStreamMetrics(\n",
    "    progressSeq: Seq[org.apache.spark.sql.streaming.StreamingQueryProgress],\n",
    "    printLatenciesFlag: Boolean = true,\n",
    "    printTransformWithStateMetricsFlag: Boolean = false\n",
    "  ): Unit = {\n",
    "    progressSeq.foreach { progress =>\n",
    "      val batchId = progress.batchId\n",
    "      val processedRows = progress.processedRowsPerSecond\n",
    "      println(s\"\\nBatchId: $batchId | ProcessedRows/s: $processedRows\")\n",
    "\n",
    "      val (latenciesOpt, stateOps) = parseProgressMetrics(progress)\n",
    "\n",
    "      if (printLatenciesFlag) {\n",
    "        printLatencies(latenciesOpt)\n",
    "      }\n",
    "      \n",
    "      if (printTransformWithStateMetricsFlag) {\n",
    "        println(\"Not Added Yet\")\n",
    "        // printTransformWithStateMetrics(stateOps)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def createKafkaTopic(topicName: String, props: java.util.Properties, partitionCount: Int = 4, replicationFactor: Short = 2): Unit = {\n",
    "    import kafkashaded.org.apache.kafka.clients.admin.{AdminClient, NewTopic}\n",
    "    import kafkashaded.org.apache.kafka.common.errors.TopicExistsException\n",
    "\n",
    "    val adminClient = AdminClient.create(props)\n",
    "    val newTopic = new NewTopic(topicName, partitionCount, replicationFactor)\n",
    "    try {\n",
    "      adminClient.createTopics(java.util.Collections.singleton(newTopic)).all().get()\n",
    "      println(s\"Kafka topic '$topicName' created successfully.\")\n",
    "    } catch {\n",
    "      case e: java.util.concurrent.ExecutionException if e.getCause.isInstanceOf[TopicExistsException] =>\n",
    "        println(s\"Kafka topic '$topicName' already exists.\")\n",
    "      case e: Exception =>\n",
    "        println(s\"Error creating Kafka topic '$topicName': ${e.getMessage}\")\n",
    "        throw e\n",
    "    } finally {\n",
    "      adminClient.close()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def deleteKafkaTopic(topicName: String, props: java.util.Properties): Unit = {\n",
    "    import kafkashaded.org.apache.kafka.clients.admin.AdminClient\n",
    "    val adminClient = AdminClient.create(props)\n",
    "    adminClient.deleteTopics(java.util.Collections.singleton(topicName)).all().get()\n",
    "    println(s\"Kafka topic '$topicName' deleted successfully.\")\n",
    "    adminClient.close()\n",
    "  }\n",
    "\n",
    "  def getLastProgress(query: org.apache.spark.sql.streaming.StreamingQuery): Option[org.apache.spark.sql.streaming.StreamingQueryProgress] = {\n",
    "    Option(query.lastProgress)\n",
    "  }\n",
    "\n",
    "  def stopStreamAfterBatchesCollectProgress(\n",
    "    spark: SparkSession,\n",
    "    stream: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row],\n",
    "    queryName: String,\n",
    "    maxBatches: Int,\n",
    "    trigger: org.apache.spark.sql.streaming.Trigger,\n",
    "    writeStreamOptions: Map[String, String],\n",
    "    runId: String\n",
    "  ): Seq[org.apache.spark.sql.streaming.StreamingQueryProgress] = {\n",
    "    val df = stream\n",
    "    val triggerTypeHeader = trigger match {\n",
    "      case t if t.toString.toLowerCase.contains(\"realtime\") => (\"triggertype\", \"realtime\")\n",
    "      case t if t.toString.toLowerCase.contains(\"processingtime\") => (\"triggertype\", \"processingtime0\")\n",
    "      case _ => (\"triggertype\", \"unknown\")\n",
    "    }\n",
    "    import org.apache.spark.sql.functions._\n",
    "    val headersCol = array(\n",
    "      struct(lit(triggerTypeHeader._1).as(\"key\"), lit(triggerTypeHeader._2).cast(\"binary\").alias(\"value\")),\n",
    "      struct(lit(\"runId\").as(\"key\"), lit(runId).cast(\"binary\").alias(\"value\")),\n",
    "      struct(lit(\"batchId\").as(\"key\"), current_batch_id().cast(\"string\").cast(\"binary\").alias(\"value\"))\n",
    "    )\n",
    "    val dfWithHeader = df.withColumn(\"headers\", headersCol)\n",
    "    val writeStream = dfWithHeader.writeStream\n",
    "      .format(\"kafka\")\n",
    "      .queryName(queryName)\n",
    "      .trigger(trigger)\n",
    "    writeStreamOptions.foreach { case (k, v) => writeStream.option(k, v) }\n",
    "    val query = writeStream.outputMode(\"update\").start()\n",
    "    var lastBatchId = -1L\n",
    "    var batchCount = 0\n",
    "    val progressBuffer = scala.collection.mutable.Buffer[org.apache.spark.sql.streaming.StreamingQueryProgress]()\n",
    "    while (query.isActive && batchCount < maxBatches) {\n",
    "      getLastProgress(query).foreach { s =>\n",
    "        if (s.batchId > lastBatchId) {\n",
    "          lastBatchId = s.batchId\n",
    "          batchCount += 1\n",
    "          progressBuffer += s\n",
    "          println(\"\\nPretty JSON for last StreamingQueryProgress:\")\n",
    "          println(Json.prettyPrint(Json.parse(s.json)))\n",
    "        }\n",
    "      }\n",
    "      Thread.sleep(200)\n",
    "    }\n",
    "    if (query.isActive) query.stop()\n",
    "    progressBuffer.toSeq\n",
    "  }\n",
    "  \n",
    "  def readAndFilterKafkaBatch(\n",
    "    spark: SparkSession,\n",
    "    kafkaBatchReadOptions: Map[String, String],\n",
    "    runId: String\n",
    "  ): org.apache.spark.sql.DataFrame = {\n",
    "    import org.apache.spark.sql.functions.{expr, col, from_json, unix_millis, lit}\n",
    "\n",
    "    val kafkaBatchDF = {\n",
    "      val reader = spark.read.format(\"kafka\")\n",
    "      kafkaBatchReadOptions.foreach { case (k, v) => reader.option(k, v) }\n",
    "      reader.load()\n",
    "    }\n",
    "\n",
    "    kafkaBatchDF.filter(\n",
    "      expr(s\"exists(headers, h -> h.key = 'runId' AND h.value = CAST('$runId' AS BINARY))\")\n",
    "    )\n",
    "    .withColumn(\"triggertype\", expr(\"filter(headers, h -> h.key = 'triggertype')[0].value\").cast(\"string\"))\n",
    "    .withColumn(\"batchId\", expr(\"filter(headers, h -> h.key = 'batchId')[0].value\").cast(\"string\").cast(\"int\"))\n",
    "    .withColumn(\"data\", from_json(col(\"value\").cast(\"string\"), SensorDataGenerator.sensorSchema))\n",
    "    .withColumn(\"sink-timestamp\", unix_millis(col(\"timestamp\")))\n",
    "    .withColumn(\"source-timestamp\", unix_millis(col(\"data.timestamp\")))\n",
    "    .withColumn(\"latency\", col(\"sink-timestamp\") - col(\"source-timestamp\")) // talk about how we calculate latency\n",
    "    .withColumn(\"runId\", lit(runId))\n",
    "    .select(\"data.*\", \"triggertype\", \"latency\", \"runId\", \"batchId\")\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "HelperFunctions",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
