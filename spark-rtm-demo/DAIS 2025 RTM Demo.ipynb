{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "960f402e-4db5-47cd-b654-3d93e47faf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DAIS RTM Demo\n",
    "* Small Demo that demonstrates the low latency benefits of RealTimeMode vs MicroBatchMode\n",
    "  * MicroBatchMode is the defacto/default mode in structured Streaming\n",
    "  * RealTimeMode is the new low latency mode in spark to allow for sub second streaming \n",
    "* In this demo we will do the following\n",
    "  * Use Spark Rate Source to generate data\n",
    "  * Apply Transformwithstate to do stateful operations on the data\n",
    "  * Write to Kafka with both MultiBatch mode (MBM) and RealTime Mode (RTM)\n",
    "  * Calculate the latency differences between the 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3949d73-dc55-4fb4-84c6-da3e08a92847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load resource files\n",
    "* SensorDataGenerator - Uses spark nate rate source to generate records\n",
    "* EnvironemntalMonitorListProcessor - TransformWithState Operator (Stateful Operation)\n",
    "* HelperFunctions - Functions to simplify Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e5cee9f-3005-4f4f-89f3-77201453a5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./resources/SensorDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4df7ab-c040-4758-87e9-b0f0401bc78b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./resources/EnvironmentalMonitorListProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273ab513-c61e-487b-aefe-b83366985307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./resources/HelperFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "330d5d4a-a0bc-4de2-b103-0966484c4448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Rate Source\n",
    "* Use spark native rate source to generate records\n",
    "* Set to generate 200 rows per second\n",
    "* timestamp col is both the record generation timestamp and used for sensor timestamp\n",
    "  * Effectively our \"source timestamp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "580d5d0c-55a7-4fa0-a579-9f0567f8dd5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">import com.databricks.dais2025.SensorDataGenerator\n",
       "val stream: org.apache.spark.sql.DataFrame = [sensor_id: string, city: string ... 6 more fields]\n",
       "val runId: String = rtmRunIDMkQrhU\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "rowsPerSecond": "200"
       },
       "data": "<div class=\"ansiout\">import com.databricks.dais2025.SensorDataGenerator\nval stream: org.apache.spark.sql.DataFrame = [sensor_id: string, city: string ... 6 more fields]\nval runId: String = rtmRunIDMkQrhU\n</div>",
       "datasetInfos": [
        {
         "name": "stream",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "sensor_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "city",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "location",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "timestamp",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "temperature",
            "nullable": false,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "humidity",
            "nullable": false,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "co2_level",
            "nullable": false,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "pm25_level",
            "nullable": false,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import com.databricks.dais2025.SensorDataGenerator\n",
    "\n",
    "// note don't increase rps too high it will affect performance, this is setup to be a demonstration not benchmarking\n",
    "// Default is 200 rowspersecond, can be changed via widget\n",
    "val stream = SensorDataGenerator.createStream(spark, dbutils.widgets.get(\"rowsPerSecond\").toInt, 8) \n",
    "\n",
    "// Used to track data in kafka\n",
    "val runId = s\"rtmRunID${scala.util.Random.alphanumeric.take(6).mkString}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7983ff3-45e0-4dd9-ab28-29df9dd6a7ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply TransformFormWithState to Stream\n",
    "* On the stream we apply [transformwithstate](https://docs.databricks.com/aws/en/stateful-applications/) operator to calculate the state of the senors per city and create alerts based on thresholds set in there\n",
    "  * group by City\n",
    "  * construct the columns we want to send to kafka as json string string column called \"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404417ea-8ba2-491f-8fdb-5a5711133ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import com.databricks.dais2025.tws.EnvironmentalMonitorListProcessor\n",
    "import org.apache.spark.sql.streaming.{OutputMode, TimeMode}\n",
    "import org.apache.spark.sql.functions.{col, struct, to_json, array, lit}\n",
    "import com.databricks.dais2025.MyStructs._\n",
    "\n",
    "val twsStream = stream\n",
    "  .as[Input]\n",
    "  .groupByKey(x => x.city)\n",
    "  .transformWithState(\n",
    "    new EnvironmentalMonitorListProcessor(), \n",
    "    TimeMode.ProcessingTime(), \n",
    "    OutputMode.Update()\n",
    "  )\n",
    "  .as[Output]\n",
    "  .withColumn(\"value\", to_json(struct(\n",
    "    col(\"sensor_id\"),\n",
    "    col(\"location\"),\n",
    "    col(\"city\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"temperature\"),\n",
    "    col(\"humidity\"),\n",
    "    col(\"co2_level\"),\n",
    "    col(\"pm25_level\"),\n",
    "    col(\"hourly_avg_temp\"),\n",
    "    col(\"daily_avg_temp\"),\n",
    "    col(\"temperature_trend\"),\n",
    "    col(\"high_temp_count\"),\n",
    "    col(\"alerts\")\n",
    "  )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "176cc6ad-6bfa-4c6c-a349-dd38a77f9cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Shuffle Partitions\n",
    "* Use 8 Shuffle partitions for shuffle stage (TWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe59013-ebf1-4c97-a922-6d3bdb1437b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23f412b6-bdac-430e-8c93-d1e649866894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Kafka Topic to Write to\n",
    "* This is setup for oetrta Kafka to bused on e2-demo-field-eng\n",
    "* Modify the kafka props if environment changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585704d9-df90-4bf7-8adf-794f81663b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "val topicName = s\"daisrtm2025${runId}\"\n",
    "val bootstrapServers = dbutils.secrets.get(\"oetrta\", \"kafka-bootstrap-servers-tls\")\n",
    "val kafkaProps = {\n",
    "  val props = new java.util.Properties()\n",
    "    props.put(\"bootstrap.servers\", bootstrapServers)\n",
    "    props.put(\"security.protocol\", \"SSL\")\n",
    "  props\n",
    "}\n",
    "val partitionCount = 4\n",
    "\n",
    "HelperFunctions.createKafkaTopic(\n",
    "  topicName=topicName, props=kafkaProps,\n",
    "  partitionCount=partitionCount, \n",
    "  replicationFactor=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "032271f8-cfa2-466b-ad95-90e048412b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## WriteStream Options\n",
    "* These are just our spark writestream options we will be using\n",
    "* Please update checkpointLocation(via widget) base path to a volume you have access to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b665bfa-b171-4dba-9333-6494634bb8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "val checkpointLocationRaw = dbutils.widgets.get(\"checkpointLocation\")\n",
    "val checkpointLocation = if (checkpointLocationRaw.endsWith(\"/\")) checkpointLocationRaw else checkpointLocationRaw.stripSuffix(\"/\")\n",
    "\n",
    "val writeStreamOptions = Map(\n",
    "  \"kafka.bootstrap.servers\" -> bootstrapServers,\n",
    "  \"kafka.security.protocol\" -> \"SSL\",\n",
    "  \"topic\" -> topicName,\n",
    "  \"checkpointLocation\" -> s\"${checkpointLocation}/${runId}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b89de55-c54f-4581-954f-077ee16fb715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run Stream in MBM Mode\n",
    "* First do 20 batches of Mbm (Microbatch Mode) \n",
    "  * This is the defacto mode in spark structured streaming\n",
    "* Use ProcessingTime of 0 seconds so it runs as fast as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1839cb0c-d654-440d-bdc7-435fc6b8fe0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "val queryProgressBatch = HelperFunctions.stopStreamAfterBatchesCollectProgress(\n",
    "  spark=spark,\n",
    "  stream = twsStream,\n",
    "  queryName = \"RTMDemo\",\n",
    "  maxBatches = 20,\n",
    "  trigger = Trigger.ProcessingTime(\"0 seconds\"), //default\n",
    "  writeStreamOptions = writeStreamOptions,\n",
    "  runId = runId\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2738cb0-2e97-409b-a215-885735eb8e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MBM No Latency Metrics\n",
    "* MBM doesnt have Latency Metrics in the query progress after a batch\n",
    "* We have to get from reading from kafka and subtracting source and sink timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7b542a-bf33-42fd-b85b-00c38a93f9ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "HelperFunctions.printRTMStreamMetrics(queryProgressBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b351812-ca67-4631-802a-ccac0e26df76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Now Run In RealTime Mode\n",
    "* We set trigger interval to 60 seconds (Default is 5 minutes)\n",
    "  * This means that we checkpoint every 60 seconds, we only attempt to checkpoint then\n",
    "  * If we used the default of 5 minutes the p99 would look better as well since less time is spent checkpointing\n",
    "* We run 5 batches here, roughly a total of 5 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbcc79b4-ccf7-45f5-89da-f71c81f0bfbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "val queryProgressRTM = HelperFunctions.stopStreamAfterBatchesCollectProgress(\n",
    "  spark = spark,\n",
    "  stream = twsStream,\n",
    "  queryName = \"RTMDemo\",\n",
    "  maxBatches = 5,\n",
    "  trigger = Trigger.RealTime(\"60 seconds\"),\n",
    "  writeStreamOptions = writeStreamOptions,\n",
    "  runId = runId\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbcca1e-34cd-446b-a5f4-aa64b6c8793d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## RTM Latency Metrics\n",
    "* Real Time Mode has latency metrics, most importantly e2eLatency which gives us how long a record took from souce to get to sink (in this case kafka)\n",
    "* [Latency Metrics Info](https://docs.databricks.com/aws/en/structured-streaming/real-time#use-streamingqueryprogress)\n",
    "* Note, there is generally some overhead on the first initial batch of realtime mode(and mbm) hence why we run multiple benches for demoing to show consistent low latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d312f110-5575-485d-a8a0-9daa5aca894e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\n",
       "BatchId: 10 | ProcessedRows/s: 216.2871590067313\n",
       "Latencies:\n",
       "  E2E Latency (ms):            P50=30       P90=2651     P95=6020     P99=8712    \n",
       "  Processing Latency (ms):     P50=30       P90=1778     P95=2004     P99=2153    \n",
       "  Source Queuing Latency (ms): P50=0        P90=685      P95=4042     P99=6719    \n",
       "\n",
       "BatchId: 11 | ProcessedRows/s: 201.22908165793865\n",
       "Latencies:\n",
       "  E2E Latency (ms):            P50=26       P90=47       P95=49       P99=657     \n",
       "  Processing Latency (ms):     P50=26       P90=47       P95=49       P99=151     \n",
       "  Source Queuing Latency (ms): P50=0        P90=0        P95=1        P99=491     \n",
       "\n",
       "BatchId: 12 | ProcessedRows/s: 199.864621683644\n",
       "Latencies:\n",
       "  E2E Latency (ms):            P50=26       P90=46       P95=49       P99=170     \n",
       "  Processing Latency (ms):     P50=26       P90=46       P95=48       P99=51      \n",
       "  Source Queuing Latency (ms): P50=0        P90=0        P95=1        P99=72      \n",
       "\n",
       "BatchId: 13 | ProcessedRows/s: 200.09578702250994\n",
       "Latencies:\n",
       "  E2E Latency (ms):            P50=26       P90=46       P95=49       P99=237     \n",
       "  Processing Latency (ms):     P50=26       P90=46       P95=49       P99=60      \n",
       "  Source Queuing Latency (ms): P50=0        P90=0        P95=1        P99=146     \n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">\nBatchId: 10 | ProcessedRows/s: 216.2871590067313\nLatencies:\n  E2E Latency (ms):            P50=30       P90=2651     P95=6020     P99=8712    \n  Processing Latency (ms):     P50=30       P90=1778     P95=2004     P99=2153    \n  Source Queuing Latency (ms): P50=0        P90=685      P95=4042     P99=6719    \n\nBatchId: 11 | ProcessedRows/s: 201.22908165793865\nLatencies:\n  E2E Latency (ms):            P50=26       P90=47       P95=49       P99=657     \n  Processing Latency (ms):     P50=26       P90=47       P95=49       P99=151     \n  Source Queuing Latency (ms): P50=0        P90=0        P95=1        P99=491     \n\nBatchId: 12 | ProcessedRows/s: 199.864621683644\nLatencies:\n  E2E Latency (ms):            P50=26       P90=46       P95=49       P99=170     \n  Processing Latency (ms):     P50=26       P90=46       P95=48       P99=51      \n  Source Queuing Latency (ms): P50=0        P90=0        P95=1        P99=72      \n\nBatchId: 13 | ProcessedRows/s: 200.09578702250994\nLatencies:\n  E2E Latency (ms):            P50=26       P90=46       P95=49       P99=237     \n  Processing Latency (ms):     P50=26       P90=46       P95=49       P99=60      \n  Source Queuing Latency (ms): P50=0        P90=0        P95=1        P99=146     \n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "HelperFunctions.printRTMStreamMetrics(queryProgressRTM)\n",
    "// discuss numbers below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "943a96f3-1efc-4129-9977-d121f0dec842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mbm Latency Numbers\n",
    "* To get the Latency numbers of Mbm, we can batch read from kafka and subtract the source timestamp (we discussed earlier) and the timestamp that kafka adds to message when recieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed46ccfe-b05f-4af6-b939-f569ffe2d364",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763358048391}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{avg, expr, percentile_approx, lit, dense_rank}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "// Read kafka as batch and filter for unique RunId we added to Kafka Header\n",
    "val filteredKafkaDf = HelperFunctions.readAndFilterKafkaBatch(\n",
    "  spark,\n",
    "  Map(\n",
    "    \"kafka.bootstrap.servers\" -> bootstrapServers,\n",
    "    \"kafka.security.protocol\" -> \"SSL\",\n",
    "    \"includeHeaders\" -> \"true\",\n",
    "    \"subscribe\" -> topicName,\n",
    "    \"startingOffsets\" -> \"earliest\",\n",
    "    \"endingOffsets\" -> \"latest\"\n",
    "  ),\n",
    "  runId\n",
    ").withColumn(\"batchnumberfortriggertype\", dense_rank().over(Window.partitionBy(\"triggertype\").orderBy(\"batchId\")))\n",
    "\n",
    "// Lets take a quick look at the actual data\n",
    "display(filteredKafkaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32138ed6-f5f2-4694-ba8a-e2baa8f0c33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "// Calculate latency numbers and p90's per batch\n",
    "\n",
    "val latencyNumbers = filteredKafkaDf.groupBy(\"triggertype\", \"runId\", \"batchId\", \"batchnumberfortriggertype\")\n",
    "    .agg(\n",
    "      avg(\"latency\").alias(\"mean_latency\"),\n",
    "      percentile_approx(expr(\"latency\"), lit(0.5), lit(100000)).alias(\"p50_latency\"),\n",
    "      percentile_approx(expr(\"latency\"), lit(0.95), lit(100000)).alias(\"p95_latency\"),\n",
    "      percentile_approx(expr(\"latency\"), lit(0.99), lit(100000)).alias(\"p99_latency\")\n",
    "    )\n",
    "    .orderBy(\"triggertype\", \"batchId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10330c1c-9309-4a06-998f-9160875cb9b7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763359208077}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>triggertype</th><th>runId</th><th>batchId</th><th>batchnumberfortriggertype</th><th>mean_latency</th><th>p50_latency</th><th>p95_latency</th><th>p99_latency</th></tr></thead><tbody><tr><td>processingtime0</td><td>rtmRunIDMkQrhU</td><td>0</td><td>1</td><td>29833.510869565216</td><td>30051</td><td>31701</td><td>31882</td></tr><tr><td>processingtime0</td><td>rtmRunIDMkQrhU</td><td>1</td><td>2</td><td>18044.399258160236</td><td>18038</td><td>33199</td><td>34549</td></tr><tr><td>processingtime0</td><td>rtmRunIDMkQrhU</td><td>2</td><td>3</td><td>1556.1492063492065</td><td>1564</td><td>2266</td><td>2328</td></tr><tr><td>processingtime0</td><td>rtmRunIDMkQrhU</td><td>3</td><td>4</td><td>1764.0641711229946</td><td>1773</td><td>2210</td><td>2292</td></tr><tr><td>processingtime0</td><td>rtmRunIDMkQrhU</td><td>4</td><td>5</td><td>1456.2871287128712</td><td>1453</td><td>2139</td><td>2196</td></tr><tr><td>processingtime0</td><td>rtmRunIDMkQrhU</td><td>5</td><td>6</td><td>1065.775147928994</td><td>1068</td><td>1446</td><td>1482</td></tr><tr><td>processingtime0</td><td>rtmRunIDMkQrhU</td><td>6</td><td>7</td><td>1031.0448717948718</td><td>1030</td><td>1386</td><td>1417</td></tr><tr><td>processingtime0</td><td>rtmRunIDMkQrhU</td><td>7</td><td>8</td><td>1001.8516129032258</td><td>1000</td><td>1351</td><td>1378</td></tr><tr><td>processingtime0</td><td>rtmRunIDMkQrhU</td><td>8</td><td>9</td><td>998.8607594936709</td><td>994</td><td>1358</td><td>1380</td></tr><tr><td>processingtime0</td><td>rtmRunIDMkQrhU</td><td>9</td><td>10</td><td>1048.3885350318471</td><td>1040</td><td>1411</td><td>1432</td></tr><tr><td>realtime</td><td>rtmRunIDMkQrhU</td><td>10</td><td>1</td><td>678.4889434889435</td><td>30</td><td>6016</td><td>8700</td></tr><tr><td>realtime</td><td>rtmRunIDMkQrhU</td><td>11</td><td>2</td><td>38.44758230030375</td><td>26</td><td>49</td><td>657</td></tr><tr><td>realtime</td><td>rtmRunIDMkQrhU</td><td>12</td><td>3</td><td>29.974558070378325</td><td>26</td><td>49</td><td>170</td></tr><tr><td>realtime</td><td>rtmRunIDMkQrhU</td><td>13</td><td>4</td><td>31.369593925387917</td><td>26</td><td>49</td><td>254</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "processingtime0",
         "rtmRunIDMkQrhU",
         0,
         1,
         29833.510869565216,
         30051,
         31701,
         31882
        ],
        [
         "processingtime0",
         "rtmRunIDMkQrhU",
         1,
         2,
         18044.399258160236,
         18038,
         33199,
         34549
        ],
        [
         "processingtime0",
         "rtmRunIDMkQrhU",
         2,
         3,
         1556.1492063492065,
         1564,
         2266,
         2328
        ],
        [
         "processingtime0",
         "rtmRunIDMkQrhU",
         3,
         4,
         1764.0641711229946,
         1773,
         2210,
         2292
        ],
        [
         "processingtime0",
         "rtmRunIDMkQrhU",
         4,
         5,
         1456.2871287128712,
         1453,
         2139,
         2196
        ],
        [
         "processingtime0",
         "rtmRunIDMkQrhU",
         5,
         6,
         1065.775147928994,
         1068,
         1446,
         1482
        ],
        [
         "processingtime0",
         "rtmRunIDMkQrhU",
         6,
         7,
         1031.0448717948718,
         1030,
         1386,
         1417
        ],
        [
         "processingtime0",
         "rtmRunIDMkQrhU",
         7,
         8,
         1001.8516129032258,
         1000,
         1351,
         1378
        ],
        [
         "processingtime0",
         "rtmRunIDMkQrhU",
         8,
         9,
         998.8607594936709,
         994,
         1358,
         1380
        ],
        [
         "processingtime0",
         "rtmRunIDMkQrhU",
         9,
         10,
         1048.3885350318471,
         1040,
         1411,
         1432
        ],
        [
         "realtime",
         "rtmRunIDMkQrhU",
         10,
         1,
         678.4889434889435,
         30,
         6016,
         8700
        ],
        [
         "realtime",
         "rtmRunIDMkQrhU",
         11,
         2,
         38.44758230030375,
         26,
         49,
         657
        ],
        [
         "realtime",
         "rtmRunIDMkQrhU",
         12,
         3,
         29.974558070378325,
         26,
         49,
         170
        ],
        [
         "realtime",
         "rtmRunIDMkQrhU",
         13,
         4,
         31.369593925387917,
         26,
         49,
         254
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "triggertype",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "runId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "batchId",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "batchnumberfortriggertype",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "mean_latency",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "p50_latency",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "p95_latency",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "p99_latency",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "// processingtime0 is MBM mode and realtime is realtime mode batches\n",
    "// discuss them (might be very sligh variation of the e2e latency numbers for realtime mode here vs streamingqueryprogress)\n",
    "display(latencyNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3110f58a-7878-4b92-a916-2878e92e6cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>triggertype</th><th>p99_latency</th></tr></thead><tbody><tr><td>processingtime0</td><td>2270</td></tr><tr><td>realtime</td><td>211</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "processingtime0",
         2270
        ],
        [
         "realtime",
         211
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "triggertype",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "p99_latency",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%scala\nvar dfsLen = 0;\n{\n  var dfs = Array[Any]()\n  implicit def display(df: Any) {\n    dfs = dfs :+ df\n  }\n\n  // run user code\n// p99 graph mbm vs rtm (ignore 1st batch of each triggertype)\n// ignore first 2 batches for both as mbm has skewed batch time on it as well\n\ndisplay(\n  filteredKafkaDf\n    .where(\"batchnumberfortriggertype not in (1,2)\")\n    .groupBy(\"triggertype\")\n    .agg(\n      percentile_approx(expr(\"latency\"), lit(0.99), lit(10000)).alias(\"p99_latency\")\n    )\n)\n\n  if (dfs.length > 0) {\n    val userGenerateDf = dfs(0).asInstanceOf[org.apache.spark.sql.DataFrame]\n    userGenerateDf.createOrReplaceTempView(\"DatabricksViewf365886\")\n  }\n  dfsLen = dfs.length\n}\nif (dfsLen > 0) {\n  try {\n    display(sql(\"\"\"WITH q AS (select * from DatabricksViewf365886) SELECT `triggertype`,SUM(`p99_latency`) `column_5f337cab405`,`triggertype` FROM q GROUP BY `triggertype`\"\"\"))\n   } finally {\n  // cleaning up the view helps us not pollute the name space\n   spark.sql(\"drop view if exists DatabricksViewf365886\")\n  }\n} else {\n  displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n}\n",
       "commandTitle": "p99 latency",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "triggertype",
             "id": "column_5f337cab408"
            },
            "x": {
             "column": "triggertype",
             "id": "column_5f337cab404"
            },
            "y": [
             {
              "column": "p99_latency",
              "id": "column_5f337cab405",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_5f337cab405": {
             "name": "p99_latency",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "title": {
             "text": "Trigger Type"
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "Latency (ms)"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "0770ce31-3ed4-47be-a63a-4bbb1efc4786",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 28.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "triggertype",
           "type": "column"
          },
          {
           "column": "triggertype",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "triggertype",
           "type": "column"
          },
          {
           "alias": "column_5f337cab405",
           "args": [
            {
             "column": "p99_latency",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "triggertype",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "// p99 graph mbm vs rtm (ignore 1st batch of each triggertype)\n",
    "// ignore first 2 batches for both as mbm has skewed batch time on it as well\n",
    "\n",
    "display(\n",
    "  filteredKafkaDf\n",
    "    .where(\"batchnumberfortriggertype not in (1,2)\")\n",
    "    .groupBy(\"triggertype\")\n",
    "    .agg(\n",
    "      percentile_approx(expr(\"latency\"), lit(0.99), lit(10000)).alias(\"p99_latency\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ff30db-cd4f-47c9-a0fa-ca379d7c671e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "// Clean up kafka topic when done\n",
    "HelperFunctions.deleteKafkaTopic(topicName, kafkaProps)\n",
    "// delete checkpoint location\n",
    "dbutils.fs.rm(s\"${checkpointLocation}/${runId}\", True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7578492502657607,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DAIS 2025 RTM Demo",
   "widgets": {
    "checkpointLocation": {
     "currentValue": "/Volumes/main/neilp/checkpoints/daisRtmdemo",
     "nuid": "e59e704e-c127-452f-9028-f2f9d2cff82a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/main/neilp/checkpoints/daisRtmdemo/",
      "label": null,
      "name": "checkpointLocation",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "/Volumes/main/neilp/checkpoints/daisRtmdemo/",
      "label": null,
      "name": "checkpointLocation",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "rowsPerSecond": {
     "currentValue": "200",
     "nuid": "d20b2e93-4b52-4209-943e-b2e33c1284db",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "200",
      "label": null,
      "name": "rowsPerSecond",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "200",
      "label": null,
      "name": "rowsPerSecond",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
