{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a687905-40b7-4cb3-9799-79f39aa8d266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install Faker\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b391d6-32ad-46c9-90c2-9b9dd5818082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "\n",
    "\n",
    "# Read customer ids from the table\n",
    "customers_df = spark.table(\"finance_summit.ingestion.banking_customers\").select(\"customer_id\")\n",
    "\n",
    "# Collect customer IDs to driver (assuming the dataset is not huge)\n",
    "customer_ids = [row.customer_id for row in customers_df.collect()]\n",
    "\n",
    "# Select every 10th customer\n",
    "selected_customer_ids = customer_ids[9::10]  # 0-based index, so 9 is the 10th\n",
    "\n",
    "# Initialize Faker and campaign details\n",
    "fake = Faker()\n",
    "campaign_names = [\"Email\", \"Mobile\", \"Facebook\", \"Instagram\", \"Google Ads\"]\n",
    "outcomes = [\"no interaction\", \"clicked\", \"accepted offer\", \"rejected offer\"]\n",
    "\n",
    "# Generate campaign data\n",
    "campaign_data = []\n",
    "for idx, customer_id in enumerate(selected_customer_ids):\n",
    "    if customer_id % 1000 == 0:\n",
    "        print(f\"Generating data for customer {customer_id}\") \n",
    "    campaign_id = 1000000 + idx\n",
    "    campaign_name = random.choice(campaign_names)\n",
    "    outcome = random.choice(outcomes)\n",
    "    campaign_data.append({\n",
    "        \"campaign_id\": campaign_id,\n",
    "        \"customer_id\": customer_id,\n",
    "        \"campaign_name\": campaign_name,\n",
    "        \"outcome\": outcome\n",
    "    })\n",
    "\n",
    "# Create pandas DataFrame\n",
    "campaign_df = pd.DataFrame(campaign_data)\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "campaign_spark_df = spark.createDataFrame(campaign_df)\n",
    "\n",
    "# Write the final dataset to the target Delta table\n",
    "campaign_spark_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"finance_summit.ingestion.campaign\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b67725-dfac-4e35-9636-3cf399f6486e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id, hash, udf\n",
    "from pyspark.sql.types import StringType, LongType\n",
    "import random\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read customer ids from the table\n",
    "customers_df = spark.table(\"finance_summit.ingestion.banking_customers\").select(\"customer_id\")\n",
    "\n",
    "# Add a synthetic partition column\n",
    "num_partitions = 100  # Adjust as needed for your cluster size\n",
    "customers_df = customers_df.withColumn(\"partition_id\", (hash(\"customer_id\") % num_partitions).cast(\"int\"))\n",
    "\n",
    "# Assign row numbers within each partition\n",
    "window_spec = Window.partitionBy(\"partition_id\").orderBy(monotonically_increasing_id())\n",
    "customers_df = customers_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Filter every 10th customer\n",
    "filtered_df = customers_df.filter(customers_df.row_num % 10 == 0)\n",
    "\n",
    "# UDFs for campaign_name and outcome\n",
    "campaign_names = [\"Email\", \"Mobile\", \"Facebook\", \"Instagram\", \"Google Ads\"]\n",
    "outcomes = [\"no interaction\", \"clicked\", \"accepted offer\", \"rejected offer\"]\n",
    "\n",
    "@udf(StringType())\n",
    "def random_campaign_name():\n",
    "    return random.choice(campaign_names)\n",
    "\n",
    "@udf(StringType())\n",
    "def random_outcome():\n",
    "    return random.choice(outcomes)\n",
    "\n",
    "# Add campaign_id, campaign_name, and outcome columns\n",
    "df_with_campaign = filtered_df.withColumn(\n",
    "    \"campaign_id\", (1000000 + filtered_df.row_num).cast(LongType())\n",
    ").withColumn(\n",
    "    \"campaign_name\", random_campaign_name()\n",
    ").withColumn(\n",
    "    \"outcome\", random_outcome()\n",
    ").select(\n",
    "    \"campaign_id\", \"customer_id\", \"campaign_name\", \"outcome\"\n",
    ")\n",
    "\n",
    "# Write the final dataset to the target Delta table\n",
    "df_with_campaign.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"finance_summit.ingestion.campaign\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_marketing_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
